# SecEval: A Comprehensive Benchmark for Evaluating Cybersecurity Knowledge of Foundation Models

SecEval is the first benchmark specifically created for evaluating cybersecurity knowledge in Foundation Models. It offers over 2000 multiple-choice questions across 9 domains: Software Security, Application Security, System Security, Web Security, Cryptography, Memory Safety, Network Security, and PenTest.
SecEval generates questions by prompting OpenAI GPT4 with authoritative sources such as open-licensed textbooks, official documentation, and industry guidelines and standards. The generation process is meticulously crafted to ensure the dataset meets rigorous quality, diversity, and impartiality criteria. Explore our dataset and detailed methodology in our [research paper](paper_placeholder.html). You can explore samples by visiting [explore](explore.html). 

