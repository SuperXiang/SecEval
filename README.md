# SecEval: A Comprehensive Benchmark for Evaluating Cybersecurity Knowledge of Foundation Models

SecEval is the first benchmark specifically created for evaluating cybersecurity knowledge in Foundation Models. It offers over 2000 multiple-choice questions across 9 domains: Software Security, Application Security, System Security, Web Security, Cryptography, Memory Safety, Network Security, and PenTest.
SecEval generates questions by prompting OpenAI GPT4 with authoritative sources such as open-licensed textbooks, official documentation, and industry guidelines and standards. The generation process is meticulously crafted to ensure the dataset meets rigorous quality, diversity, and impartiality criteria. You can explore our dataset and detailed methodology in our [research paper](paper_placeholder.html), or explore samples by visiting [explore](explore.html). 



## Table of Contents

- [Leaderboard](#leaderboard)
- [Dataset](#dataset)
- [Evaluate](#evaluate)
- [Submit](#submit)
- [Licenses](#licenses)
- [Citation](#citation)



## Leaderboard

#### Zero-shot



#### Five-shot



## Download

- You can download the zip file of the dataset by running

  ```
  
  ```
  

- Or you can load the dataset from [Huggingface]()

  ```
  
  ```

## Evaluate



## Submit



## Licenses



## Citation

